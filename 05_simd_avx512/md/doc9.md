
---

# ç¬¬ 5 ç« 

## å°†æœ€ä¼˜åŠ¨æ€ Batch è°ƒåº¦å™¨

## è½åœ°åˆ° Triton / TensorRT / vLLM çš„çœŸå®åšæ³•

---

## 5.1 å…ˆè¯´ç»“è®ºï¼ˆå·¥ç¨‹çœŸç›¸ï¼‰

| æ¡†æ¶                          | èƒ½å¦å®Œæ•´æ”¯æŒ   | éš¾ç‚¹             |
| --------------------------- | -------- | -------------- |
| **Triton Inference Server** | âœ… æœ€å®¹æ˜“    | å¤šå®ä¾‹ + åŠ¨æ€ batch |
| **TensorRT**                | âš ï¸ éœ€å¤–å±‚è°ƒåº¦ | æœ¬èº«ä¸ç®¡è¯·æ±‚         |
| **vLLM**                    | âš ï¸ å¯æ·±åº¦èåˆ | è°ƒåº¦é€»è¾‘æå¤æ‚        |

ğŸ‘‰ **æœ€ä¼˜è°ƒåº¦å™¨å‡ ä¹ä¸€å®šåœ¨â€œæ¨ç†å¼•æ“ä¹‹å¤–â€**

---

## 5.2 ç»Ÿä¸€æŠ½è±¡ï¼šè°ƒåº¦å™¨åº”è¯¥æ”¾åœ¨å“ªä¸€å±‚ï¼Ÿ

### æ¨ç†ç³»ç»Ÿåˆ†å±‚ï¼ˆéå¸¸é‡è¦ï¼‰

```
[ Client ]
    â†“
[ Request Queue ]   â† ğŸ‘ˆ è°ƒåº¦å™¨åœ¨è¿™é‡Œ
    â†“
[ Inference Engine ]
    â†“
[ Hardware (AMX / GPU) ]
```

**åŸåˆ™**ï¼š

> âŒ ä¸æ”¹ kernel
> âŒ ä¸ä¾µå…¥ç®—å­
> âœ… æ§åˆ¶â€œä½•æ—¶ã€ç”¨è°ã€è·‘å¤šå°‘ batchâ€

---

## 5.3 Triton Inference Serverï¼šæœ€ä½³å®éªŒå¹³å°

### 5.3.1 Triton çš„å¤©ç„¶ä¼˜åŠ¿

* åŸç”Ÿæ”¯æŒï¼š

  * Dynamic batching
  * Multiple model instances
  * CPU / GPU åŒæ—¶éƒ¨ç½²
* æ¶æ„æ˜¯ **æ˜¾å¼è¯·æ±‚é˜Ÿåˆ—**

ğŸ‘‰ **Triton æœ¬èº«å°±æ˜¯ä¸€ä¸ªè°ƒåº¦æ¡†æ¶**

---

### 5.3.2 åœ¨ Triton ä¸­å®ç° AMX + GPU åŒè·¯å¾„

#### Step 1ï¼šéƒ¨ç½²ä¸¤ä¸ª model instance

```text
model/
 â”œâ”€â”€ amx_instance/
 â”‚    â””â”€â”€ instance_group { kind: KIND_CPU }
 â””â”€â”€ gpu_instance/
      â””â”€â”€ instance_group { kind: KIND_GPU }
```

---

#### Step 2ï¼šå…³é—­ Triton å†…ç½® dynamic batchingï¼ˆå…³é”®ï¼‰

```text
dynamic_batching {
  max_queue_delay_microseconds: 0
}
```

å› ä¸ºï¼š

> **æˆ‘ä»¬è¦è‡ªå·±ç®—â€œç­‰ä¸ç­‰å€¼ä¸å€¼â€**

---

### 5.3.3 æ’å…¥è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼ˆæ ¸å¿ƒï¼‰

ä½ç½®ï¼š

* **Scheduler / Backend API**
* æˆ– External Request Router

#### è°ƒåº¦é€»è¾‘

```cpp
on_request_arrival(req):
    enqueue(req)

periodic_tick():
    B = queue.size()

    if B < B1:
        dispatch(amx_instance)
    else if B < B2:
        if marginal_gain(B) > wait_cost():
            wait()
        else:
            dispatch(gpu_instance)
    else:
        dispatch(gpu_instance)
```

---

### 5.3.4 Triton çš„çœŸå®å·¥ç¨‹æ³¨æ„ç‚¹

âœ… ä¼˜ç‚¹ï¼š

* Triton è‡ªåŠ¨åšå†…å­˜ç®¡ç†
* CPU/GPU å¹¶è¡Œ
* æ˜“äº A/B æµ‹è¯•

âš ï¸ é™åˆ¶ï¼š

* å•æ¨¡å‹å¤š instance çš„å‚æ•°å…±äº«å¤æ‚
* AMX kernel éœ€è‡ªå·±å†™ï¼ˆONEDNN / custom backendï¼‰

---

## 5.4 TensorRTï¼šè°ƒåº¦å™¨å¿…é¡»åœ¨â€œå¤–é¢â€

### 5.4.1 ä¸ºä»€ä¹ˆ TensorRT ä¸é€‚åˆå†…åµŒï¼Ÿ

TensorRT çš„å®šä½æ˜¯ï¼š

> **â€œç»™å®š batchï¼Œæœ€å¿«ç®—å®Œâ€**

å®ƒ**å®Œå…¨ä¸å…³å¿ƒ**ï¼š

* è¯·æ±‚æ’é˜Ÿ
* batch å½¢æˆ
* SLA

ğŸ‘‰ **è°ƒåº¦å¿…é¡»åœ¨ TensorRT ä¹‹ä¸Š**

---

### 5.4.2 æ­£ç¡®æ¶æ„ï¼ˆå·¥ä¸šå®è·µï¼‰

```
[ Request Queue ]
        â†“
[ Custom Scheduler ]
        â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ TensorRT AMXâ”‚ TensorRT GPUâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.4.3 AMX + TensorRT çš„ç°å®æ–¹æ¡ˆ

#### äº‹å®

* TensorRT **ä¸æ”¯æŒ AMX**
* CPU æ¨ç†ä¸€èˆ¬ç”¨ï¼š

  * oneDNN
  * OpenVINO

---

#### å®é™…è½åœ°æ–¹æ¡ˆ

| è·¯å¾„      | å¼•æ“           |
| ------- | ------------ |
| å° batch | oneDNN + AMX |
| å¤§ batch | TensorRT GPU |

---

### 5.4.4 è°ƒåº¦å…³é”®ç‚¹

1ï¸âƒ£ **æå‰ warm-up GPU**
é¿å… batch åˆ‡æ¢å¯¼è‡´å†·å¯åŠ¨

2ï¸âƒ£ **GPU launch amortization threshold**
æ¥è‡ª profilingï¼Œä¸æ˜¯çŒœçš„

3ï¸âƒ£ **å¼ºåˆ¶ flush tail request åˆ° AMX**

---

### 5.4.5 TensorRT çš„çœŸå®é™åˆ¶

âŒ ä¸æ”¯æŒè·¨ batch èåˆ
âŒ åŠ¨æ€ shape æˆæœ¬é«˜
âŒ å¤š stream è°ƒåº¦å¤æ‚

ğŸ‘‰ æ‰€ä»¥ **TensorRT â‰  è°ƒåº¦ç³»ç»Ÿ**

---

## 5.5 vLLMï¼šæœ€å¤æ‚ï¼Œä¹Ÿæœ€å€¼å¾—åš

vLLM æ˜¯ **æœ€éš¾æ”¹ï¼Œä½†æ”¶ç›Šæœ€å¤§** çš„ç³»ç»Ÿã€‚

---

## 5.5.1 vLLM çš„è°ƒåº¦ç°çŠ¶ï¼ˆäº‹å®ï¼‰

vLLM å·²ç»æœ‰ï¼š

* Continuous batching
* Token-level scheduling
* KV cache ç®¡ç†

ä½†ï¼š

> **å®ƒå‡è®¾â€œåªç”¨ GPUâ€**

---

## 5.5.2 åœ¨ vLLM ä¸­æ’å…¥ AMX çš„æ­£ç¡®æ–¹å¼

### ä¸è¦åŠ¨æ¨¡å‹ kernelï¼ˆè¿™æ˜¯é™·é˜±ï¼‰

âŒ ä¸è¦æ”¹ attention kernel
âŒ ä¸è¦åœ¨ decode å†…åˆ†å‰

---

### æ­£ç¡®æ’å…¥ç‚¹ï¼š**Prefill é˜¶æ®µ**

LLM æ¨ç†ï¼š

```
Prefill (heavy GEMM)
Decode  (memory bound)
```

---

### 5.5.3 æ–° pipelineï¼ˆå·¥ä¸šå¯è¡Œï¼‰

```
Incoming requests
        â†“
Dynamic batch scheduler
        â†“
Small prefill â†’ CPU AMX
Large prefill â†’ GPU Tensor Core
        â†“
Decode â†’ GPU only
```

ğŸ‘‰ **AMX åªåš prefill**

---

### 5.5.4 ä¸ºä»€ä¹ˆè¿™åœ¨æ•°å­¦ä¸Šæ˜¯æœ€ä¼˜çš„ï¼Ÿ

* Prefillï¼š

  * è®¡ç®—å¯†é›†
  * batch æ•ˆåº”æ˜æ˜¾
* Decodeï¼š

  * batch è¶Šå¤§è¶Šæ…¢
  * GPU memory bound

---

### 5.5.5 vLLM ä¸­çš„çœŸå®æ”¹åŠ¨ç‚¹

| æ¨¡å—        | æ”¹åŠ¨               |
| --------- | ---------------- |
| Scheduler | å¢åŠ  AMX path      |
| Executor  | æ”¯æŒ CPU execution |
| KV Cache  | CPU â†’ GPU è½¬ç§»     |

âš ï¸ è¿™æ˜¯ **vLLM hack çº§æ”¹é€ **

---

## 5.6 ç»Ÿä¸€è°ƒåº¦å™¨æ¥å£è®¾è®¡ï¼ˆå¼ºçƒˆå»ºè®®ï¼‰

ä¸ºäº†è·¨ Triton / TensorRT / vLLM å¤ç”¨ï¼š

```cpp
struct ExecutionPath {
    double estimate_latency(batch);
    void execute(batch);
};

class OptimalScheduler {
    vector<ExecutionPath> paths;
    Decision schedule(queue_state);
};
```

---

## 5.7 å·¥ä¸šçº§ç»éªŒæ€»ç»“ï¼ˆè¡€æ³ªï¼‰

### âŒ å¸¸è§å¤±è´¥æ¨¡å¼

* ç›²ç›®è¿½æ±‚ GPU åˆ©ç”¨ç‡
* å¿½è§† p99
* æŠŠ batch å½“ç›®æ ‡

---

### âœ… æˆåŠŸç³»ç»Ÿçš„å…±åŒç‚¹

* Tail request æ°¸è¿œæœ‰â€œé€ƒç”Ÿé€šé“â€ï¼ˆAMXï¼‰
* GPU åªåƒâ€œå·²ç»è‚¥çš„ batchâ€
* å†³ç­–åŸºäº **è¾¹é™…æ”¶ç›Š**

---

## 5.8 ä¸€å¥è¯ç»ˆææ€»ç»“

> **æ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒä¸æ˜¯â€œç®—å¾—å¤šå¿«â€ï¼Œ
> è€Œæ˜¯â€œä»€ä¹ˆæ—¶å€™ä¸è¯¥ç­‰â€ã€‚**

æŠŠè¿™å¥—è°ƒåº¦å™¨å¡è¿› Triton / TensorRT / vLLMï¼Œ
ä½ å¾—åˆ°çš„ä¸æ˜¯ 5% ä¼˜åŒ–ï¼Œè€Œæ˜¯ï¼š

* p99 ç›´æ¥è…°æ–©
* GPU åˆ©ç”¨ç‡ä¸Šå‡
* ç³»ç»Ÿè¿›å…¥â€œå¯è¯æ˜æœ€ä¼˜â€çš„çŠ¶æ€

---
